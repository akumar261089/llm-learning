
# Azure OpenAI LLM Configuration

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import httpx
import os
from dotenv import load_dotenv

# Load environment variables from `.env` file
load_dotenv()

# Azure OpenAI Configuration
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT", "https://<your-endpoint>.openai.azure.com/")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "<deployment-name>")
AZURE_API_VERSION = os.getenv("AZURE_API_VERSION", "2023-07-01-preview")
AZURE_SUBSCRIPTION_KEY = os.getenv("AZURE_SUBSCRIPTION_KEY", "<your-api-key>")
# Initialize FastAPI
app = FastAPI()

# Define Request and Response Models
class Message(BaseModel):
    role: str  # e.g., 'system', 'user', 'assistant'
    content: str  # Actual message/content

class LLMRequest(BaseModel):
    prompt: List[Message]  # List of Message objects forming the conversation history
    max_tokens: int = 100
    temperature: float = 0.7
    top_p: float = 0.95

class LLMResponse(BaseModel):
    completion: str  # The response generated by the LLM


# Endpoint to interact with Azure OpenAI LLM
@app.post("/azure-llm/", response_model=LLMResponse)
async def get_llm_response(request: LLMRequest):
    """
    Interacts with Azure OpenAI LLM using user-defined parameters.
    """
    # Endpoint URL for Azure OpenAI
    url = f"{AZURE_OPENAI_ENDPOINT}openai/deployments/{AZURE_OPENAI_DEPLOYMENT}/chat/completions?api-version={AZURE_API_VERSION}"

    # Headers for Azure OpenAI API
    headers = {
        "Content-Type": "application/json",
        "api-key": AZURE_SUBSCRIPTION_KEY,
    }

    # Payload sent to Azure OpenAI
    payload = {
        "messages": [message.dict() for message in request.prompt],  # Convert list of Message objects to dicts
        "max_tokens": request.max_tokens,
        "temperature": request.temperature,
        "top_p": request.top_p,
    }

    try:
        # Sending the request to Azure OpenAI endpoint
        async with httpx.AsyncClient() as client:
            response = await client.post(url, json=payload, headers=headers)
            response.raise_for_status()  # Raise an exception for HTTP errors

            # Process the response
            data = response.json()
            completion = data.get("choices", [{}])[0].get("message", {}).get("content", "")

            # Return a structured response
            return LLMResponse(completion=completion)

    except httpx.HTTPStatusError as err:
        # Raise HTTP exceptions for client/server errors from Azure OpenAI
        raise HTTPException(
            status_code=err.response.status_code,
            detail=f"Azure OpenAI API error: {err.response.text}"
        )
    except Exception as e:
        # Catch any other general exceptions
        raise HTTPException(
            status_code=500,
            detail=f"An unexpected error occurred: {str(e)}"
        )


# To run the application, use `uvicorn main:app --reload`