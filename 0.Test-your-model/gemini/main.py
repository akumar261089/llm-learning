# Gemini LLM Configuration
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import httpx
import os
from dotenv import load_dotenv

# Load environment variables from `.env` file
load_dotenv()

# Gemini API Configuration
GEMINI_API_ENDPOINT = os.getenv(
    "GEMINI_API_ENDPOINT", "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "<your-api-key>")

# Initialize FastAPI
app = FastAPI()

# Define Request and Response Models
class Message(BaseModel):
    text: str  # The user message to be sent as a prompt

class LLMRequest(BaseModel):
    contents: List[Message]  # List of message contents forming the conversation details
    max_tokens: int = 100  # Optional - max tokens for response (if supported by Gemini)
    temperature: float = 0.7  # Optional - controls randomness (if supported by Gemini)

class LLMResponse(BaseModel):
    completion: str  # Completion/response generated by the LLM

# Endpoint to interact with Gemini LLM
@app.post("/gemini-llm/", response_model=LLMResponse)
async def get_llm_response(request: LLMRequest):
    """
    Interacts with Gemini API using user-defined parameters.
    """
    url = GEMINI_API_ENDPOINT
    # Headers for Gemini API
    headers = {
        "Content-Type": "application/json",
        "X-goog-api-key": GEMINI_API_KEY,
    }

    # Payload sent to Gemini API
    payload = {
        "contents": [{"parts": [{"text": message.text} for message in request.contents]}],
        # Add optional fields if Gemini supports them
        # "max_tokens": request.max_tokens,
        # "temperature": request.temperature,
    }

    try:
        # Send the request to the Gemini endpoint
        async with httpx.AsyncClient() as client:
            response = await client.post(url, json=payload, headers=headers)
            response.raise_for_status()  # Raise an exception for HTTP errors
            
            # Process the response
            data = response.json()
            completion = data.get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "")
            
            # Return structured response
            return LLMResponse(completion=completion)
    except httpx.HTTPStatusError as err:
        # Raise HTTP exceptions for client/server errors from Gemini
        raise HTTPException(
            status_code=err.response.status_code,
            detail=f"Gemini API error: {err.response.text}"
        )
    except Exception as e:
        # Catch other general exceptions
        raise HTTPException(
            status_code=500,
            detail=f"An unexpected error occurred: {str(e)}"
        )

# To run the application, use `uvicorn main:app --reload`