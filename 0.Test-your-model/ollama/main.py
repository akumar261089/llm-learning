# Ollama LLM Configuration
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import requests  # Use requests library
import os
from dotenv import load_dotenv

# Load environment variables from `.env` file
load_dotenv()

# Ollama Configuration
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://ollamahost:11434/api/generate")  # Update to your Ollama endpoint
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3.2:latest")  # Default model

# Initialize FastAPI
app = FastAPI()

# Define Request and Response Models
class Message(BaseModel):
    role: str  # e.g., 'system', 'user', 'assistant'
    content: str  # Actual message/content

class LLMRequest(BaseModel):
    prompt: List[Message]  # List of Message objects forming the conversation history
    max_tokens: int = 100
    temperature: float = 0.7
    stream: bool = False

class LLMResponse(BaseModel):
    completion: str  # The response generated by the LLM

# Endpoint to interact with Ollama LLM
@app.post("/ollama-llm/", response_model=LLMResponse)
def get_llm_response(request: LLMRequest):
    """
    Interacts with Ollama LLM using user-defined parameters.
    """
    # Construct the prompt from the conversation history
    conversation = "\n".join([f"{message.role}: {message.content}" for message in request.prompt])

    # Prepare the payload
    payload = {
        "model": OLLAMA_MODEL,
        "prompt": conversation,
        "max_tokens": request.max_tokens,
        "temperature": request.temperature,
        "stream": request.stream,  # Must be lowercase `true/false` for JSON
    }

    headers = {
        "Content-Type": "application/json",
    }

    # Debugging output
    print(f"Payload to Ollama: {payload}")
    print(f"Using Ollama URL: {OLLAMA_URL}")

    try:
        # Send the POST request using requests
        print("Sending request to Ollama...")
        response = requests.post(OLLAMA_URL, json=payload, headers=headers)

        # Log the response
        print(f"Response status code: {response.status_code}")
        print(f"Response body: {response.text}")

        # Raise an exception for non-successful responses
        if response.status_code != 200:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Ollama API error: {response.text}"
            )

        # Parse response JSON
        data = response.json()
        # Extract the response content
        completion = data.get("response", "")
        return LLMResponse(completion=completion)

    except requests.exceptions.RequestException as e:
        # Handle connection-related or request-related errors
        print(f"RequestException: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"An error occurred while connecting to the Ollama API: {str(e)}"
        )

    except Exception as e:
        # Catch all other exceptions
        print(f"Unexpected Error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"An unexpected error occurred: {str(e)}"
        )